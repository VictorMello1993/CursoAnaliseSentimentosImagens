# -*- coding: utf-8 -*-
"""Deteção de emoções em vídeos 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Y0cIQ5_bDEuaeOJ-S2e2fJIVubFMMFO

# Detecção de emoções em vídeos

### Etapa 1 Importando as bibliotecas
"""

import cv2
import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
import zipfile

cv2.__version__

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow
tensorflow.__version__

"""### Etapa 2 - Conectando com o Google Drive e acessando os arquivos"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)
path = "/content/drive/My Drive/Material.zip"
zip_object = zipfile.ZipFile(file=path, mode="r")
zip_object.extractall("./")
zip_object.close

"""### Etapa 3 - Carregando o modelo 2"""

from tensorflow.keras.models import load_model
model = load_model('Material/modelo_02_expressoes.h5')

"""### Etapa 4 - Carregando o vídeo"""

arquivo_video = 'Material/Videos/video_teste06.MOV'
cap = cv2.VideoCapture(arquivo_video)

conectado, video = cap.read()

#Imprimindo o resultado do carregamento de vídeo. O valor True indica que o vídeo foi carregado com sucesso.
#video.shape - Indica as dimensões do vídeo e o número de canais de imagens (ou frames)
print(conectado, video.shape) #360 x 640 com 3 canais de frames RGB

"""### Etapa 5 - Redimensionando o tamanho (opcional)

É recomendado quando o tamanho do vídeo é muito grande. Se o vídeo tiver a resolução muito alta então irá levar mais tempo para concluir o processamento.
"""

redimensionar = True

#Define o tamanho da largura (máxima) do vídeo a ser salvo. A altura será proporcional e é definida nos cálculos abaixo:
largura_maxima = 600

if(redimensionar and video.shape[1] > largura_maxima):
  #Largura e altura proporcionais (mantendo a proporção do vídeo original) para que a imagem não fique com a aparência esticada
  proporcao = video.shape[1] / video.shape[0] #Dividindo a altura pela largura para obter a proporção
  video_largura = largura_maxima
  video_altura = int(video_largura / proporcao)
else: #Caso contrário, o vídeo será mantido em suas dimensões originais
  video_largura = video.shape[1]
  video_altura = video.shape[0]

"""### Etapa 6 - Definindo as configurações do vídeo"""

nome_arquivo = 'resultado_video_teste06.avi'

#Definição do codec
#FourCC é um arquivo de 4 bytes usado para especificar o codec de vídeo. A lista de códigos disponíveis pode ser encontrada no site fourcc.
#Codecs mais usados: XVID, MP4V, MJPG, DIVX, X264...
#Por exemplo: para salvar em formato mp4 utiliza-se o codec mp4v (o nome do arquivo também precisa possuir a extensão .mp4)
fourcc = cv2.VideoWriter_fourcc(*'XVID') #Salvando o vídeo em formato avi

#Para deixar o vídeo um pouco mais lento, pode-se diminuir a taxa de fps para 20 para facilitar na detecção
fps = 20

saida_video = cv2.VideoWriter(nome_arquivo, fourcc, fps, (video_largura, video_altura))

"""Mais exemplos de outras configurações com o fourcc que é possível usar: https://www.programcreek.com/python/example/89348/cv2.VideoWriter_fourcc

### Etapa 7 - Processamento do vídeo e gravação do resultado

*   Nessa parte, ocorre a detecção de emoções frame por frame, utilizando o mesmo modelo da rede neural pré-treinado;

*   OBS: o modelo foi treinado para detecção de emoções mais para rostos de frente, centralizado e levemente inclinado, pois na maioria das imagens existentes na base de dados utilizada, os rostos estavam sendo visualizados dessa forma. Então o modelo não está preparado para detectar quando os rostos estão muito inclinados, muito longes uns aos outros ou muito pequenos. Por isso que o haarcascade muitas vezes não consegue identificar esses rostos dependendo de um frame específico

*   Porém, existem 3 opções para que isso seja possível:
  *   Ajustar os parâmetros na função detectMultiScale()
  *   Adicionar mais um haarcascade para detectar rostos de perfil (porém, é necessário fazer uma verificação para evitar os possíveis detecções duplicadas)
  *   Usar a biblioteca Dblib para melhor precisão (mas é mais lento do que openCV)


* Fatores que podem influenciar na detecção de emoção:
  * Qualidade do vídeo em função do seu formato ou do seu codec. Como os vídeos passam às vezes por uma compressão mais pesada para reduzir o tamanho do arquivo, a perda de qualidade pode ser mais nítida em alguns frames do que os outros;
  * Nível de compressão, conforme dito no primeiro item

Portanto, é recomendável que o vídeo tenha alta resolução para melhores resultados.
"""

from tensorflow.keras.preprocessing.image import img_to_array

unica_face = False #Variável que indica que o vídeo só possui uma única face. Assim, será exibido um gráfico de probabilidades no canto
                  #Se atribuir para false, significa que o algoritmo irá detectar emoções de todas as faces, mas sem exibir gráfico

haarcascade_faces = 'Material/haarcascade_frontalface_default.xml' # Detectando faces utilizando haarcascade

# define os tamanhos para as fontes
fonte_pequena, fonte_media = 0.4, 0.7

fonte = cv2.FONT_HERSHEY_SIMPLEX

expressoes = ["Raiva", "Nojo", "Medo", "Feliz", "Triste", "Surpreso", "Neutro"] 

while (cv2.waitKey(1) < 0):
    conectado, frame = cap.read()
    
    if not conectado:
        break  # se ocorreu um problema ao carregar a imagem então interrompe o programa

    t = time.time() # tempo atual, antes de iniciar (vamos utilizar para calcular quanto tempo levou para executar as operações)
    
    # frame_video = np.copy(frame) # faz uma copia do frame do video

    if redimensionar: # se redimensionar = True então redimensiona o frame para os novos tamanhos
      frame = cv2.resize(frame, (video_largura, video_altura)) 

    face_cascade = cv2.CascadeClassifier(haarcascade_faces)
    cinza = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # converte para escala de cinza para acelerar no processo de treinamento da rede neural
    faces = face_cascade.detectMultiScale(cinza,scaleFactor=1.2, minNeighbors=5,minSize=(30,30)) #Configurando o fator de escala das faces

    if len(faces) > 0:
        for (x, y, w, h) in faces:

            frame = cv2.rectangle(frame,(x,y),(x+w,y+h+10),(255,50,50),2) # desenha retângulo ao redor da face

            roi = cinza[y:y + h, x:x + w]      # extrai apenas a região de interesse (ROI) que é onde contém o rosto 
            roi = cv2.resize(roi, (48, 48))    # antes de passar para rede neural, redimensiona para o tamanho das imagens de treinamento
            roi = roi.astype("float") / 255.0  # normalização
            roi = img_to_array(roi)            # converte para array para que a rede possa processar
            roi = np.expand_dims(roi, axis=0)  # muda o shape da array

            # faz a predição - calcula as probabilidades
            result = model.predict(roi)[0]     
            print(result)                
            if result is not None:
              if unica_face:
                for(index, (emotion, prob)) in enumerate(zip(expressoes, result)):
                  text = "{}: {:.2f}%".format(emotion, prob * 100) 
                  barra = int(prob * 150) #Calcula o tamanho da barra baseado no valor da probabilidade calculado
                  cv2.rectangle(frame, (7, (index * 18) + 7), (barra, (index * 18) + 18), (200, 250, 20), -1)
                  cv2.putText(frame, text, (15, (index * 18) + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0, 0, 0), 1, cv2.LINE_AA)      

              resultado = np.argmax(result) # encontra a emoção com maior probabilidade
              cv2.putText(frame,expressoes[resultado],(x,y-10), fonte, fonte_media,(255,255,255),1,cv2.LINE_AA) # escreve a emoção acima do rosto
                
    # tempo processado = tempo atual (time.time()) - tempo inicial (t)
    cv2.putText(frame, " frame processado em {:.2f} segundos".format(time.time() - t), (20, video_altura-20), fonte, fonte_pequena, (250, 250, 250), 0, lineType=cv2.LINE_AA)

    cv2_imshow(frame) 
    saida_video.write(frame) # grava o frame atual

print("Terminou")
saida_video.release() 
cv2.destroyAllWindows()