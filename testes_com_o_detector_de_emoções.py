# -*- coding: utf-8 -*-
"""Testes com o detector de emoções

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m-U0eDD3G70_K1d_1XMIQVvIPQgQSwJV

**Etapa 1 - Importando as bibliotecas**
"""

import cv2
import numpy as np
import pandas as pd
from google.colab.patches import cv2_imshow
import zipfile

"""%tensorflow_version 2.x"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array
# %tensorflow_version
tensorflow.__version__

"""**Etapa 2 - Conectando com o Drive e acessando os arquivos**"""

from google.colab import drive
drive.mount('/content/gdrive')

from google.colab import drive
drive.mount('/content/drive')

"""**Etapa 3 - Descompactando todos os arquivos**"""

path = "/content/drive/My Drive/Material.zip"
zip_object = zipfile.ZipFile(file = path, mode = "r")
zip_object.extractall('./')
zip_object.close

"""Carregando imagem"""

imagem = cv2.imread('Material/testes/20200418_174830.jpg')
cv2_imshow(imagem)

imagem.shape #Resolução da imagem

# largura = imagem.shape[0]
# altura = imagem.shape[1]

# #Se a imagem for muito grande, reduzir para 400 x 300
# largura = 400
# altura = 300
# dim = (altura, largura)

# #Redimensionando a imagem
# resized = cv2.resize(imagem, dim, interpolation = cv2.INTER_AREA)

# print('Imagem redimensionada: ', resized.shape)

# cv2_imshow(resized)

"""## Testando o detector

**Carregamento dos modelos**
"""

cascade_faces = 'Material/haarcascade_frontalface_default.xml'
caminho_modelo = 'Material/modelo_01_expressoes.h5'
face_detection = cv2.CascadeClassifier(cascade_faces)
classificador_emocoes = load_model(caminho_modelo, compile = False)
expressoes = ["Raiva", "Nojo", "Medo", "Feliz", "Triste", "Surpreso", "Neutro"]

"""**Detecção de faces**"""

original = imagem.copy()
faces = face_detection.detectMultiScale(original, scaleFactor = 1.1,
                                        minNeighbors = 5, minSize = (20,20))

faces #Detectando as coordenadas de uma face em uma imagem

#Armazenando as coordenadas de uma face
fX = faces[0][0]
fY = faces[0][1]
fW = faces[0][2] 
fH = faces[0][3] 
print(fX)
print(fY)
print(fW)
print(fH)

len(faces)

faces.shape

"""**Extração do ROI (region of interest)**"""

cinza = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY) #Convertendo uma imagem colorida para tons de cinza
cv2_imshow(cinza)

cinza.shape

"""Com o *for* são realizadas as etapas para cada face detectada:


*   Extração do ROI (region of interest)
*   Redimensionamento
*   Normalização
*   Previsões e resultado
"""

#Extração de um ROI com uma única imagem
roi = cinza[fY:fY + fH, fX:fX + fW] #Reduzindo a dimensão da imagem para treinamento rápido da rede neural
# roi = cinza[105:105 + 209, 53:53 + 209] #Reduzindo a dimensão da imagem para treinamento rápido da rede neural

#Extração de ROIs com múltiplas imagens de um arquivo.
# for(x, y, w, h) in faces:
#   #Em cada face é utilizada as suas respectivas coordenadas (onde inicia cada face) e a largura e altura para extrair o ROI
#   roi = cinza[y:y + h, x:x + w]

#   #Redimensionando o ROI de cada face para 48 x 48
#   roi = cv2.resize(roi, (48,48)) 
#   cv2_imshow(roi)

#   #Normalização
#   roi = roi.astype("float") / 255
#   roi = img_to_array(roi)
#   roi = np.expand_dims(roi, axis = 0)

#   #Previsões
#   preds = classificador_emocoes.predict(roi)[0]
#   print(preds)

#   #Emoção detectada em cada face
#   emotion_probability = np.max(preds)
#   print(emotion_probability)

#   print(preds.argmax())
#   label = expressoes[preds.argmax()]

#   #Mostra o resultado na tela para o rosto de cada face
#   cv2.putText(original, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.65,
#               (0,0,255), 2, cv2.LINE_AA)
#   cv2.rectangle(original, (x, y), (x + w, y + h), (0, 0, 255), 2)

cv2_imshow(original)
# cv2_imshow(resized)

cv2_imshow(roi)

roi.shape #Novas dimensões da imagem após o tratamento

roi #Extraindo a matriz de pixels de uma imagem

roi = cv2.resize(roi, (48,48)) #Redimensionando o ROI para 40 x 40
cv2_imshow(roi)

roi.shape

roi.dtype

roi = roi.astype('float') #Convertendo a matriz de pixels para float, para detecção de faces na escala de 0 a 1 (normalização)
roi.dtype

roi

roi = roi / 255

roi

roi = img_to_array(roi)

roi

roi.shape

roi = np.expand_dims(roi, axis = 0)
roi.shape

"""## Previsões"""

#Efetuando previsões das emoções de um ROI
preds = classificador_emocoes.predict(roi)[0]

preds

len(preds) #detectando o número de atributos classes

emotion_probability = np.max(preds)
emotion_probability #Obtendo a probabilidade para a classe com maior grau de intensidade

preds.argmax() #Obtendo a posição do preds (emoções) com maior grau de intensidade. No caso do feliz, está na posição 3

label = expressoes[preds.argmax()]
label

"""## Resultados"""

cv2.putText(original, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0,0,255), 2, cv2.LINE_AA)
cv2.rectangle(original, (fX, fY), (fX + fW, fY + fH), (0, 0, 255), 2) #Desenhando um retângulo identificador de faces
cv2_imshow(original)

# cv2.putText(resized, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0,0,255), 2, cv2.LINE_AA)
# cv2.rectangle(resized, (fX, fY), (fX + fW, fY + fH), (0, 0, 255), 2) #Desenhando um retângulo identificador de faces
# cv2_imshow(resized)

probabilidades = np.ones((250, 300, 3), dtype='uint8') * 255
probabilidades

probabilidades.shape

cv2_imshow(original)
if len(faces) == 1:
  for(i, (emotion, prob)) in enumerate(zip(expressoes, preds)): #Exibindo as probabilidades de cada estado emocional em uma imagem
    # print(i, emotion, prob)
    text = "{}: {:.2f}%".format(emotion, prob * 100)
    width = int(prob * 300)

    #Desenhando uma barra de progresso de cada estado emocional
    cv2.rectangle(probabilidades, (7, (i * 35) + 5), (width, (i * 35) + 35), (200, 250, 20), - 1) 
    cv2.putText(probabilidades, text, (10, (i * 35) + 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 0), 1, cv2.LINE_AA)
cv2_imshow(probabilidades)